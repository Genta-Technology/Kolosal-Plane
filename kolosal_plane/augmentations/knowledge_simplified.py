"""Simplified dataset augmentation for knowledge or data ingestion"""
from typing import Tuple, Dict
import asyncio

import polars as pl
from tqdm import tqdm
from tqdm.asyncio import tqdm_asyncio

from .knowledge import Knowledge


class SimpleKnowledge(Knowledge):
    def augmentate(self) -> Tuple[pl.DataFrame, Dict]:
        """
        Augments the dataset using a simplified approach:
          - Generate conversation starter questions from documents.
          - In each conversation round, generate an LLM response (using a built chat history)
            but save the response paired with the original user chat history (without the extra context).
          - Optionally generate a follow-up question (if not the last round) and add new rows.
        Returns:
            pl.DataFrame: A DataFrame with columns 'chat_history', 'document', and 'response'.
            int: The total number of input tokens processed by the LLM.
            int: The total number of output tokens generated by the LLM.
            int: The total number of input tokens processed by the TLM.
            int: The total number of output tokens generated by the TLM.
        """
        augmented_data = pl.DataFrame(schema={
            "chat_history": pl.List(pl.Struct([pl.Field("role", pl.Utf8), pl.Field("content", pl.Utf8)])),
            "document": pl.Utf8,
            "response": pl.Utf8
        })

        llm_input_token_count, llm_output_token_count = 0, 0
        tlm_input_token_count, tlm_output_token_count = 0, 0

        # Step 1: Generate conversation starter questions for each document
        for document in self.documents:
            # Build knowledge instructions for this document.
            built_knowledge_instructions = self.build_knowledge_instruction(
                document=document)
            self.conversation_starter_instruction = built_knowledge_instructions

            # Generate conversation starter chat histories
            chat_histories, metadata_conversation_starter = self.generate_conversation_starter()

            # Update token counts
            llm_input_token_count += metadata_conversation_starter["input_token_count"]
            llm_output_token_count += metadata_conversation_starter["output_token_count"]

            documents_data = [document] * len(chat_histories)

            augmented_data = augmented_data.vstack(pl.DataFrame({
                "chat_history": chat_histories,
                "document": documents_data,
                "response": [""] * len(chat_histories)
            }))

        # Loop for max_conversations rounds to build and expand the dataset.
        is_last = False
        for count in tqdm(range(self.max_conversations), desc="Augmenting conversations"):
            if count == self.max_conversations - 1:
                is_last = True

            # Get rows that have not been answered yet.
            temporary_augmented_data = augmented_data.filter(
                pl.col("response") == "")

            # Remove the unanswered rows from augmented_data so that we do not process them twice.
            augmented_data = augmented_data.filter(pl.col("response") != "")

            # Batch the data to catch and isolate errors
            batch_temporary_augmented_data = [
                temporary_augmented_data[i: i + self.batch_size]
                for i in range(0, len(temporary_augmented_data), self.batch_size)
            ]

            for batch in batch_temporary_augmented_data:
                batch_status = True
                try:
                    # Keep the original chat histories for saving.
                    original_chat_histories = batch["chat_history"].to_list()

                    # Build chat histories for the generation call (adds extra context for the LLM).
                    built_chat_histories = self.build_chat_histories(
                        documents=batch["document"].to_list(),
                        chat_histories=original_chat_histories
                    )

                    responses = []

                    # Generate LLM or thinking responses using the built chat histories.
                    if self.thinking_model:
                        responses, temporary_input_token_count, temporary_output_token_count = self.generate_response_thinking(
                            chat_histories=built_chat_histories)

                        # Update token counts
                        tlm_input_token_count += temporary_input_token_count
                        tlm_output_token_count += temporary_output_token_count
                    else:
                        responses, temporary_input_token_count, temporary_output_token_count = self.generate_response_llm(
                            chat_histories=built_chat_histories)

                        # Update token counts
                        llm_input_token_count += temporary_input_token_count
                        llm_output_token_count += temporary_output_token_count

                    # Save the response along with the original chat histories.
                    augmented_data = augmented_data.vstack(pl.DataFrame({
                        "chat_history": original_chat_histories,
                        "document": batch["document"],
                        "response": responses
                    }))
                except Exception as e:
                    print(
                        f"Error in handling batch responses, omitting this batch: {e}")
                    batch_status = False

                # Step 3: (If not the last round) Generate follow-up questions and expand the dataset.
                if batch_status and not is_last:
                    try:
                        questions, documents, temporary_input_token_count, temporary_output_token_count = self.generate_next_conversation(
                            llm=self.llm_model,
                            chat_histories=batch["chat_history"].to_list(),
                            responses=responses,
                            previous_documents=batch["document"].to_list()
                        )

                        # Update token counts
                        llm_input_token_count += temporary_input_token_count
                        llm_output_token_count += temporary_output_token_count

                        generated_chat_histories = []
                        generated_chat_documents = []
                        for chat_history, response, question, document in zip(
                            batch["chat_history"].to_list(),
                            responses,
                            questions,
                            documents
                        ):
                            # Append the new conversation to the original chat history.
                            new_chat_history = chat_history + [
                                {"role": "assistant", "content": response},
                                {"role": "user", "content": question}
                            ]
                            generated_chat_histories.append(new_chat_history)
                            generated_chat_documents.append(document)

                        # Save the new rows with an empty response to be processed in a later round.
                        augmented_data = augmented_data.vstack(pl.DataFrame({
                            "chat_history": generated_chat_histories,
                            "document": generated_chat_documents,
                            "response": [""] * len(generated_chat_histories)
                        }))
                    except Exception as e:
                        print(
                            f"Error in handling batch followup questions: {e}")

        # Generate metadata
        metadata = {
            "llm_input_token_count": llm_input_token_count,
            "llm_output_token_count": llm_output_token_count,
            "tlm_input_token_count": tlm_input_token_count,
            "tlm_output_token_count": tlm_output_token_count
        }

        return augmented_data, metadata


class AsyncSimpleKnowledge(Knowledge):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # These will hold your dataset and metadata when ready
        self._augmented_data: pl.DataFrame = pl.DataFrame(
            schema={
                "chat_history": pl.List(pl.Struct([pl.Field("role", pl.Utf8), pl.Field("content", pl.Utf8)])),
                "document": pl.Utf8,
                "response": pl.Utf8
            }
        )
        self._metadata = {
            "llm_input_token_count": 0,
            "llm_output_token_count": 0,
            "tlm_input_token_count": 0,
            "tlm_output_token_count": 0
        }
        self._status = "Not started"
        self._task: asyncio.Task = None
        self._lock = asyncio.Lock()  # Add lock for thread safety

    async def _run_sync_function(self, func, *args, **kwargs):
        """Helper method to run synchronous functions in the running event loop"""
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(None, lambda: func(*args, **kwargs))

    async def augmentate_async(self) -> Tuple[pl.DataFrame, Dict[str, int]]:
        try:
            self._status = "Running"
            # Step 1: Generate conversation starter questions for each document
            for document in self.documents:
                await asyncio.sleep(0)  # yield control
                built_knowledge_instructions = self.build_knowledge_instruction(
                    document=document)
                self.conversation_starter_instruction = built_knowledge_instructions

                # Generate conversation starter chat histories - run synchronously in thread pool
                starter_result = await self._run_sync_function(self.generate_conversation_starter)
                chat_histories, metadata_conversation_starter = starter_result

                # Update token counts
                async with self._lock:  # Use lock to safely update shared data
                    self._metadata["llm_input_token_count"] += metadata_conversation_starter["input_token_count"]
                    self._metadata["llm_output_token_count"] += metadata_conversation_starter["output_token_count"]

                documents_data = [document] * len(chat_histories)
                new_df = pl.DataFrame({
                    "chat_history": chat_histories,
                    "document": documents_data,
                    "response": [""] * len(chat_histories)
                })

                async with self._lock:
                    self._augmented_data = self._augmented_data.vstack(new_df)

            # Step 2: Loop for max_conversations rounds
            is_last = False
            for count in tqdm_asyncio(range(self.max_conversations), desc="Augmenting conversations"):
                await asyncio.sleep(0)
                if count == self.max_conversations - 1:
                    is_last = True

                async with self._lock:
                    temporary_augmented_data = self._augmented_data.filter(
                        pl.col("response") == "")
                    self._augmented_data = self._augmented_data.filter(
                        pl.col("response") != "")

                # Process data in batches
                batch_temporary_augmented_data = [
                    temporary_augmented_data[i: i + self.batch_size]
                    for i in range(0, len(temporary_augmented_data), self.batch_size)
                ]

                for batch in batch_temporary_augmented_data:
                    try:
                        original_chat_histories = batch["chat_history"].to_list(
                        )
                        built_chat_histories = self.build_chat_histories(
                            documents=batch["document"].to_list(),
                            chat_histories=original_chat_histories
                        )

                        # Generate response using the appropriate model - run synchronously in thread pool
                        if self.thinking_model:
                            response_result = await self._run_sync_function(
                                self.generate_response_thinking,
                                chat_histories=built_chat_histories
                            )
                            responses, temp_tlm_input, temp_tlm_output = response_result
                            async with self._lock:
                                self._metadata["tlm_input_token_count"] += temp_tlm_input
                                self._metadata["tlm_output_token_count"] += temp_tlm_output
                        else:
                            response_result = await self._run_sync_function(
                                self.generate_response_llm,
                                chat_histories=built_chat_histories
                            )
                            responses, temp_llm_input, temp_llm_output = response_result
                            async with self._lock:
                                self._metadata["llm_input_token_count"] += temp_llm_input
                                self._metadata["llm_output_token_count"] += temp_llm_output

                        # Save the response with original chat histories
                        new_df = pl.DataFrame({
                            "chat_history": original_chat_histories,
                            "document": batch["document"],
                            "response": responses
                        })

                        async with self._lock:
                            self._augmented_data = self._augmented_data.vstack(
                                new_df)

                    except Exception as e:
                        print(f"Error in batch responses: {e}")
                        continue

                    # Step 3: Generate follow-up questions if not the last round
                    if not is_last:
                        try:
                            next_convo_result = await self._run_sync_function(
                                self.generate_next_conversation,
                                llm=self.llm_model,
                                chat_histories=batch["chat_history"].to_list(),
                                responses=responses,
                                previous_documents=batch["document"].to_list()
                            )
                            questions, documents, temp_llm_input, temp_llm_output = next_convo_result

                            async with self._lock:
                                self._metadata["llm_input_token_count"] += temp_llm_input
                                self._metadata["llm_output_token_count"] += temp_llm_output

                            generated_chat_histories = []
                            generated_chat_documents = []
                            for chat_history, response, question, document in zip(
                                batch["chat_history"].to_list(
                                ), responses, questions, documents
                            ):
                                new_chat_history = chat_history + [
                                    {"role": "assistant", "content": response},
                                    {"role": "user", "content": question}
                                ]
                                generated_chat_histories.append(
                                    new_chat_history)
                                generated_chat_documents.append(document)

                            new_df = pl.DataFrame({
                                "chat_history": generated_chat_histories,
                                "document": generated_chat_documents,
                                "response": [""] * len(generated_chat_histories)
                            })

                            async with self._lock:
                                self._augmented_data = self._augmented_data.vstack(
                                    new_df)

                        except Exception as e:
                            print(
                                f"Error in generating follow-up questions: {e}")

            self._status = "Finished"
            return self._augmented_data, self._metadata

        except asyncio.CancelledError:
            self._status = "Cancelled"
            raise
        except Exception as e:
            self._status = f"Failed: {str(e)}"
            raise

    def start_augmentation(self):
        """
        Starts the augmentation process in the background.
        """
        if self._task is None or self._task.done():
            self._task = asyncio.create_task(self.augmentate_async())
        return self._task

    def get_result(self) -> Tuple[pl.DataFrame, Dict[str, int]]:
        """
        Returns the current dataset and metadata.
        """
        return self._augmented_data, self._metadata

    def cancel_augmentation(self):
        """
        Cancels the running augmentation task.
        """
        if self._task and not self._task.done():
            self._task.cancel()

    def get_status(self) -> Tuple[str, Dict]:
        """
        Returns the current status of the augmentation process.
        """
        return self._status, self._metadata

    def is_running(self) -> bool:
        """Check if the augmentation task is currently running"""
        return self._task is not None and not self._task.done() and self._status == "Running"
